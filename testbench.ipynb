{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from bidict import bidict\n",
    "import collections\n",
    "\n",
    "default_unk_token = '<UNK>'\n",
    "\n",
    "class Vocab:\n",
    "    word_to_idx: bidict\n",
    "    word_to_count: dict\n",
    "    \n",
    "    def __init__(self, word_to_idx: bidict, word_to_count: dict, unk_token=default_unk_token):\n",
    "        self.word_to_count=word_to_count\n",
    "        self.word_to_idx=word_to_idx\n",
    "        self.unk_token=unk_token\n",
    "        try: \n",
    "            self.add_type(unk_token)\n",
    "        except ValueError: \n",
    "            pass\n",
    "\n",
    "    def __str__(self): \n",
    "        return str(self.word_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "    def _get_new_word_id(self): \n",
    "        return max(self.word_to_idx.values())+1\n",
    "\n",
    "    def add_type(self, token, count=0):\n",
    "        if token in self.word_to_idx: \n",
    "            raise ValueError(f\"token '{token}' is already in the vocabulary\")\n",
    "        self.word_to_idx[token] = self._get_new_word_id()\n",
    "        self.word_to_count[token] = count\n",
    "        return token\n",
    "\n",
    "    def token_count(self):\n",
    "        return int(sum(self.word_to_count.values()))\n",
    "\n",
    "    def encode_token(self, token):\n",
    "        return self.word_to_idx.get(token, self.word_to_idx[self.unk_token])\n",
    "    \n",
    "    def encode_sent(self, sent):\n",
    "        return [self.encode_token(token) for token in sent]\n",
    "\n",
    "    def encode_batch(self, sents):\n",
    "        return [self.encode_sent(sent) for sent in sents]\n",
    "\n",
    "    def decode_token(self, token_idx):\n",
    "        return self.word_to_idx.inv[token_idx]\n",
    "\n",
    "    def decode_sent(self, sent, stringify=False):\n",
    "        tokens = [self.decode_token(token) for token in sent]\n",
    "        if stringify: tokens = ' '.join(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def decode_batch(self, sents, stringify=False):\n",
    "        return [self.decode_sent(sent, stringify) for sent in sents]\n",
    "    \n",
    "def build_vocab(flat_tokens:typing.Iterable=[], min_count=1, unk_token:str=default_unk_token):\n",
    "    \"\"\" helper method to build a vocabulary from a stream of tokens \"\"\"\n",
    "    word_to_count = collections.Counter([tok for tok in flat_tokens])\n",
    "    word_to_count = collections.Counter({w: c for w, c in word_to_count.items() if c >= min_count})\n",
    "    word_to_idx = bidict((word, i) for i, (word, count) in enumerate(word_to_count.most_common()))\n",
    "    return Vocab(word_to_idx=word_to_idx, word_to_count=word_to_count, unk_token=unk_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(bidict({'s': 0, 'a': 1, 'j': 2, 'd': 3, 'f': 4, 'l': 5, 'k': 6, 'h': 7, 'z': 8, '<UNK>': 9}),\n",
       " Counter({'s': 9,\n",
       "          'a': 9,\n",
       "          'l': 8,\n",
       "          'k': 8,\n",
       "          'j': 9,\n",
       "          'd': 9,\n",
       "          'f': 9,\n",
       "          'h': 8,\n",
       "          'z': 1,\n",
       "          '<UNK>': 0}),\n",
       " [9, 5, 1, 9, 5, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcb = build_vocab('salkjdfhaszlkjdfhaskljdfhaslkjdfhajksdfhadsjklfhaslkjdfhasdkljfhasdlfj', min_count=1)\n",
    "vcb.word_to_idx, vcb.word_to_count, vcb.encode_sent('blabla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': ['a'], 'label': 0}\n",
      "{'data': ['b'], 'label': 1}\n",
      "{'data': ['c'], 'label': 2}\n",
      "{'data': ['d'], 'label': 3}\n",
      "{'data': ['e'], 'label': 4}\n",
      "{'data': ['f'], 'label': 5}\n",
      "{'data': ['g'], 'label': 6}\n",
      "{'data': ['h'], 'label': 7}\n",
      "{'data': ['a', 'b', 'c', 'd', 'e'], 'label': (0,)}\n",
      "{'data': ['f', 'g', 'h'], 'label': (1,)}\n",
      "\n",
      "{'segments': [{'data': [0, 1], 'label_fine': (1, 1, 1)}, {'data': [2], 'label_fine': (1, 1, 2)}, {'data': [3], 'label_fine': (1, 1, 3)}], 'label_coarse': (1, 1)}\n",
      "{'segments': [{'data': [4, 5], 'label_fine': (1, 2, 4)}], 'label_coarse': (1, 2)}\n",
      "{'segments': [{'data': [6, 7], 'label_fine': (1, 3, 4)}, {'data': [8], 'label_fine': (1, 3, 5)}], 'label_coarse': (1, 3)}\n",
      "{'data': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'label': (1,)}\n",
      "\n",
      "{'segments': [{'data': [0, 1], 'label_fine': (1, 1, 1)}, {'data': [2], 'label_fine': (1, 1, 2)}, {'data': [3], 'label_fine': (1, 1, 3)}], 'label_coarse': (1, 1)}\n",
      "{'segments': [{'data': [4, 5], 'label_fine': (1, 2, 4)}], 'label_coarse': (1, 2)}\n",
      "{'segments': [{'data': [6, 7], 'label_fine': (1, 3, 4)}, {'data': [8], 'label_fine': (1, 3, 5)}], 'label_coarse': (1, 3)}\n",
      "{'data': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'label': (1,)}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import typing\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "try: \n",
    "    from . import iterator as it\n",
    "except ImportError: \n",
    "    import segmenters.iterator as it\n",
    "\n",
    "Key = typing.Union[str, int, typing.Set[typing.Union[str, int]]]\n",
    "\n",
    "class SegmentedCorpus:\n",
    "    def __init__(self, vocab: Vocab, data: typing.Iterable, \n",
    "                 segmentation: typing.Union[None, typing.Iterable, typing.List[typing.Iterable], typing.Dict[str, typing.Iterable]]=None,\n",
    "                 packed=True) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            vocab (Vocab): Vocabulary object (use build_vocab function to obtain it)\n",
    "            data (typing.Iterable): Any iterable object\n",
    "            segmentation (typing.Union[None, typing.Iterable, typing.List[typing.Iterable], typing.Dict[str, typing.Iterable]], optional): Segmentations. Can be a dict, list, or a single segmentation. Defaults to None.\n",
    "            packed (bool, optional): If true, indicates that in the provided segmentation format a single element is a tuple (segment_label, segment_size). Else, assumes segmentations are lists of labels where a consequtive sequence indicates a segment. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.packed = packed\n",
    "        if type(segmentation) == list: \n",
    "            self.segmentations = {i: s for i, s in enumerate(segmentation)}\n",
    "        elif type(segmentation) == dict: \n",
    "            self.segmentations = segmentation\n",
    "        else: \n",
    "            self.segmentations = {0: segmentation}\n",
    "\n",
    "    def list_available_segmentations(self):\n",
    "        return list(self.segmentations.keys())\n",
    "    \n",
    "    def _normalize_key(self, key: Key):\n",
    "        if key is None: \n",
    "            return key\n",
    "        if type(key) == str or type(key) == int: \n",
    "            key = set([key])\n",
    "        key = set([_ for _ in key if _ is not None])\n",
    "        for single in key: \n",
    "            if single not in self.segmentations: \n",
    "                raise KeyError(f'provided key (single) not in available segmentations ({\",\".join(self.list_available_segmentations())})')\n",
    "        return key\n",
    "    \n",
    "    def _default_segmentation(self): \n",
    "        data_len = sum(1 for _ in self.data)\n",
    "        return map(lambda i: (i, 1), range(data_len))\n",
    "    \n",
    "    def _resolve_segmentation(self, *keys): \n",
    "        \"\"\" normalizes key and returns a (packed) segmentation iterator \"\"\"\n",
    "        if keys[0] is None: \n",
    "            return self._default_segmentation()\n",
    "        else: \n",
    "            keys = [self._normalize_key(k) for k in keys if k is not None]\n",
    "            key = set.union(*keys)\n",
    "            segmentations_single = [self.segmentations[k] for k in key]\n",
    "            if self.packed: \n",
    "                # for zipping, segmentations must be unpacked\n",
    "                segmentations_single = [SegmentedCorpus._unpack(s) for s in segmentations_single]\n",
    "            segmentation = zip(*segmentations_single) # combine segmentations by zipping\n",
    "            segmentation = SegmentedCorpus._pack(segmentation) # pack again\n",
    "            return segmentation\n",
    "    \n",
    "    def _unpack(segmentation): \n",
    "        for label, size in segmentation: \n",
    "            for i in range(size): \n",
    "                yield label\n",
    "\n",
    "    def _pack(segmentation): \n",
    "        for key, group in itertools.groupby(segmentation): \n",
    "            yield key, sum(1 for _ in group)\n",
    "\n",
    "    def segments(self, key: Key):\n",
    "        segmentation = self._resolve_segmentation(key)\n",
    "        iter_data = iter(self.data)\n",
    "        for label, size in segmentation:\n",
    "            _data = [next(iter_data) for i in range(size)]\n",
    "            segment = {'data': _data, 'label': label}\n",
    "            yield segment\n",
    "\n",
    "    def segments_wrt(self, coarse:Key, fine:Key):\n",
    "        iter_data = iter(self.data)\n",
    "        seg_fine = self._resolve_segmentation(fine, coarse)\n",
    "        seg_coarse = self._resolve_segmentation(coarse)\n",
    "        iter_seg_fine = iter(seg_fine)\n",
    "        iter_seg_coarse = iter(seg_coarse)\n",
    "\n",
    "        while True: \n",
    "            try: key_coarse, size_coarse = next(iter_seg_coarse)\n",
    "            except StopIteration: break\n",
    "            segment = []\n",
    "            while size_coarse > 0: \n",
    "                key_fine, size_fine = next(iter_seg_fine)\n",
    "                segment.append({\n",
    "                    'data': [next(iter_data) for i in range(size_fine)], \n",
    "                    'label_fine': key_fine})\n",
    "                size_coarse -= size_fine\n",
    "            yield {'segments': segment, 'label_coarse': key_coarse}\n",
    "\n",
    "    def build_from_lines(lines: typing.Iterable, split_line=str.split, line_index=True, segmentation_name='line_num', **kwargs): \n",
    "        lines_split = it.RestartableMapIterator(lines, split_line)\n",
    "        lines_split_flat = it.RestartableFlattenIterator(lines_split)\n",
    "        vcb = build_vocab(lines_split_flat, **kwargs)\n",
    "        \n",
    "        segmentations={}\n",
    "        if line_index: \n",
    "            segmentations[segmentation_name] = []\n",
    "            for i, line in enumerate(lines_split):\n",
    "                segmentations[segmentation_name].append((i, len(line)))\n",
    "\n",
    "        corpus = SegmentedCorpus(vcb, lines_split_flat, segmentations, True)\n",
    "        return corpus\n",
    "\n",
    "    def build_conll(): \n",
    "        pass\n",
    "\n",
    "corpus = SegmentedCorpus.build_from_lines(['a b c d e', 'f g h'])\n",
    "for line in corpus.segments(None):\n",
    "    print(line)\n",
    "for line in corpus.segments('line_num'):\n",
    "    print(line)\n",
    "print()\n",
    "\n",
    "s0 = [1,1,1,1,1,1,1,1,1]\n",
    "s1 = [1,1,1,1,2,2,3,3,3]\n",
    "s2 = [1,1,2,3,4,4,4,4,5]\n",
    "seq = range(len(s1))\n",
    "vcb = build_vocab(seq)\n",
    "sc = SegmentedCorpus(vcb, seq, [s0, s1, s2], False)\n",
    "for seg in sc.segments_wrt((0, 1), 2): \n",
    "    print(seg)\n",
    "for seg in sc.segments(0): \n",
    "    print(seg)\n",
    "\n",
    "s0 = [(1,9)]\n",
    "s1 = [(1,4), (2,2), (3,3)]\n",
    "s2 = [(1,2), (2,1), (3,1), (4,4), (5,1)]\n",
    "seq = range(9)\n",
    "vcb = build_vocab(seq)\n",
    "sc = SegmentedCorpus(vcb, seq, [s0, s1, s2], True)\n",
    "print()\n",
    "for seg in sc.segments_wrt((0, 1), 2): \n",
    "    print(seg)\n",
    "for seg in sc.segments(0): \n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = iter(corpus.data)\n",
    "next(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
