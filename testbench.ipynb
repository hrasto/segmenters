{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from bidict import bidict\n",
    "import collections\n",
    "import nltk\n",
    "import pickle\n",
    "import itertools\n",
    "import nltk\n",
    "\n",
    "try: \n",
    "    from .iterator import *\n",
    "except ImportError: \n",
    "    from segmenters import RestartableMapIterator, RestartableFlattenIterator, RestartableBatchIterator, RestartableCallableIterator\n",
    "\n",
    "default_unk_token = '<UNK>'\n",
    "\n",
    "class Vocab:\n",
    "    word_to_idx: bidict\n",
    "    word_to_count: dict\n",
    "    \n",
    "    def __init__(self, word_to_idx: bidict, word_to_count: dict, unk_token=default_unk_token):\n",
    "        self.word_to_count=word_to_count\n",
    "        self.word_to_idx=word_to_idx\n",
    "        self.unk_token=unk_token\n",
    "        try: \n",
    "            self.add_type(unk_token)\n",
    "        except ValueError: \n",
    "            pass\n",
    "\n",
    "    def __str__(self): \n",
    "        return str(self.word_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "    def _get_new_word_id(self): \n",
    "        return max(self.word_to_idx.values())+1\n",
    "\n",
    "    def add_type(self, token, count=0):\n",
    "        if token in self.word_to_idx: \n",
    "            raise ValueError(f\"token '{token}' is already in the vocabulary\")\n",
    "        self.word_to_idx[token] = self._get_new_word_id()\n",
    "        self.word_to_count[token] = count\n",
    "        return token\n",
    "\n",
    "    def token_count(self):\n",
    "        return int(sum(self.word_to_count.values()))\n",
    "\n",
    "    def encode_token(self, token):\n",
    "        return self.word_to_idx.get(token, self.word_to_idx[self.unk_token])\n",
    "    \n",
    "    def encode_sent(self, sent):\n",
    "        return [self.encode_token(token) for token in sent]\n",
    "\n",
    "    def encode_batch(self, sents):\n",
    "        return [self.encode_sent(sent) for sent in sents]\n",
    "\n",
    "    def decode_token(self, token_idx):\n",
    "        return self.word_to_idx.inv[token_idx]\n",
    "\n",
    "    def decode_sent(self, sent, stringify=False):\n",
    "        tokens = [self.decode_token(token) for token in sent]\n",
    "        if stringify: tokens = ' '.join(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def decode_batch(self, sents, stringify=False):\n",
    "        return [self.decode_sent(sent, stringify) for sent in sents]\n",
    "    \n",
    "    def build(flat_tokens:typing.Iterable=[], min_count=1, unk_token:str=default_unk_token):\n",
    "        \"\"\" helper method to build a vocabulary from a stream of tokens \"\"\"\n",
    "        word_to_count = collections.Counter([tok for tok in flat_tokens])\n",
    "        word_to_count = collections.Counter({w: c for w, c in word_to_count.items() if c >= min_count})\n",
    "        word_to_idx = bidict((word, i) for i, (word, count) in enumerate(word_to_count.most_common()))\n",
    "        return Vocab(word_to_idx=word_to_idx, word_to_count=word_to_count, unk_token=unk_token)\n",
    "    \n",
    "#vcb = Vocab.build('salkjdfhaszlkjdfhaskljdfhaslkjdfhajksdfhadsjklfhaslkjdfhasdkljfhasdlfj', min_count=1)\n",
    "#vcb.word_to_idx, vcb.word_to_count, vcb.encode_sent('blabla')\n",
    "    \n",
    "Key = typing.Union[str, int, typing.Set[typing.Union[str, int]]]\n",
    "\n",
    "class SegmentedCorpus:\n",
    "    def __init__(self, data: typing.Iterable, \n",
    "                 segmentation: typing.Union[None, typing.Iterable, typing.List[typing.Iterable], typing.Dict[str, typing.Iterable]]=None,\n",
    "                 packed=True,\n",
    "                 vocab: Vocab=None) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            vocab (Vocab): Vocabulary object (use build_vocab function to obtain it)\n",
    "            data (typing.Iterable): Any iterable object\n",
    "            segmentation (typing.Union[None, typing.Iterable, typing.List[typing.Iterable], typing.Dict[str, typing.Iterable]], optional): Segmentations. Can be a dict, list, or a single segmentation. Defaults to None.\n",
    "            packed (bool, optional): If true, indicates that in the provided segmentation format a single element is a tuple (segment_label, segment_size). Else, assumes segmentations are lists of labels where a consequtive sequence indicates a segment. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.packed = packed\n",
    "        if type(segmentation) == list: \n",
    "            self.segmentations = {i: s for i, s in enumerate(segmentation)}\n",
    "        elif type(segmentation) == dict: \n",
    "            self.segmentations = segmentation\n",
    "        else: \n",
    "            self.segmentations = {0: segmentation}\n",
    "\n",
    "    def list_available_segmentations(self):\n",
    "        return list(self.segmentations.keys())\n",
    "    \n",
    "    def _enumerate_iterables(self): \n",
    "        if type(self.data) != list:\n",
    "            self.data = list(self.data)\n",
    "        for key in self.segmentations.keys(): \n",
    "            if type(self.segmentations[key]) != list: \n",
    "                self.segmentations[key] = list(self.segmentations[key])\n",
    "    \n",
    "    def _normalize_key(self, key: Key):\n",
    "        if key is None: \n",
    "            return key\n",
    "        if type(key) == str or type(key) == int: \n",
    "            key = set([key])\n",
    "        key = set([_ for _ in key if _ is not None])\n",
    "        for single in key: \n",
    "            if single not in self.segmentations: \n",
    "                raise KeyError(f'provided key (single) not in available segmentations ({\",\".join(self.list_available_segmentations())})')\n",
    "        return key\n",
    "    \n",
    "    def _default_segmentation(self): \n",
    "        data_len = sum(1 for _ in self.data)\n",
    "        return map(lambda i: (i, 1), range(data_len))\n",
    "    \n",
    "    def _resolve_segmentation(self, *keys): \n",
    "        \"\"\" normalizes key and returns a (packed) segmentation iterator \"\"\"\n",
    "        if keys[0] is None: \n",
    "            return self._default_segmentation()\n",
    "        else: \n",
    "            keys = [self._normalize_key(k) for k in keys if k is not None]\n",
    "            key = set.union(*keys)\n",
    "            segmentations_single = [self.segmentations[k] for k in key]\n",
    "            if self.packed: \n",
    "                # for zipping, segmentations must be unpacked\n",
    "                segmentations_single = [SegmentedCorpus._unpack(s) for s in segmentations_single]\n",
    "            segmentation = zip(*segmentations_single) # combine segmentations by zipping\n",
    "            segmentation = SegmentedCorpus._pack(segmentation) # pack again\n",
    "            return segmentation\n",
    "    \n",
    "    def _unpack(segmentation): \n",
    "        for label, size in segmentation: \n",
    "            for i in range(size): \n",
    "                yield label\n",
    "\n",
    "    def _pack(segmentation): \n",
    "        for key, group in itertools.groupby(segmentation): \n",
    "            yield key, sum(1 for _ in group)\n",
    "\n",
    "    def segments(self, key: Key):\n",
    "        segmentation = self._resolve_segmentation(key)\n",
    "        iter_data = iter(self.data)\n",
    "        for label, size in segmentation:\n",
    "            _data = [next(iter_data) for i in range(size)]\n",
    "            segment = {'data': _data, 'label': label}\n",
    "            yield segment\n",
    "\n",
    "    def segments_wrt(self, coarse:Key, fine:Key):\n",
    "        iter_data = iter(self.data)\n",
    "        seg_fine = self._resolve_segmentation(fine, coarse)\n",
    "        seg_coarse = self._resolve_segmentation(coarse)\n",
    "        iter_seg_fine = iter(seg_fine)\n",
    "        iter_seg_coarse = iter(seg_coarse)\n",
    "\n",
    "        while True: \n",
    "            try: key_coarse, size_coarse = next(iter_seg_coarse)\n",
    "            except StopIteration: break\n",
    "            segment = []\n",
    "            while size_coarse > 0: \n",
    "                key_fine, size_fine = next(iter_seg_fine)\n",
    "                segment.append({\n",
    "                    'data': [next(iter_data) for i in range(size_fine)], \n",
    "                    'label_fine': key_fine})\n",
    "                size_coarse -= size_fine\n",
    "            yield {'segments': segment, 'label_coarse': key_coarse}\n",
    "\n",
    "    def save(self, path, enumerate_iterables=True): \n",
    "        if enumerate_iterables: self._enumerate_iterables()\n",
    "        with open(path, 'wb') as f: \n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def load(path):\n",
    "        with open(path, 'rb') as f: \n",
    "            return pickle.load(f)\n",
    "\n",
    "    def build_from_lines(lines: typing.Iterable, split_line=str.split, line_index=True, min_count=1, unk_token=default_unk_token): \n",
    "        \"\"\"Build corpus from lines.\n",
    "\n",
    "        Args:\n",
    "            lines (typing.Iterable): Iterable over strings that can be split by split_line.\n",
    "            split_line (_type_, optional): Function that splits lines. Defaults to str.spl\n",
    "            line_index (bool, optional): Whether to include a line index as a segmentation. Defaults to True.\n",
    "            min_count (int, optional): Minimum word count for vocabulary building. Defaults to 1.\n",
    "            unk_token (_type_, optional): Unknown token. Defaults to '<UNK>'.\n",
    "\n",
    "        Returns:\n",
    "            SegmentedCorpus: Built corpus.\n",
    "        \"\"\"\n",
    "        lines_split = RestartableMapIterator(lines, split_line)\n",
    "        lines_split_flat = RestartableFlattenIterator(lines_split)\n",
    "        vcb = Vocab.build(flat_tokens=lines_split_flat, \n",
    "                          min_count=min_count, unk_token=unk_token)\n",
    "        lines_split_flat_idx = RestartableMapIterator(\n",
    "            lines_split_flat, vcb.encode_token)\n",
    "        segmentations={}\n",
    "        if line_index: \n",
    "            segmentations['line_num'] = []\n",
    "            for i, line in enumerate(lines_split):\n",
    "                segmentations['line_num'].append((i, len(line)))\n",
    "        corpus = SegmentedCorpus(data=lines_split_flat_idx, \n",
    "                                 segmentation=segmentations, \n",
    "                                 packed=True, vocab=vcb)\n",
    "        return corpus\n",
    "\n",
    "    def build_conll(min_count=1, unk_token=default_unk_token, *args, **kwargs): \n",
    "        \"\"\"Builds corpus from CoNLL formatted file(s). \n",
    "\n",
    "        Args:\n",
    "            min_count (int, optional): Minimum word count to consider when building vocabulary. Defaults to 1.\n",
    "            unk_token (_type_, optional): Unknown token. Defaults to '<UNK>'.\n",
    "\n",
    "        Returns:\n",
    "            SegmentedCorpus: Built corpus.\n",
    "        \"\"\"\n",
    "        reader = nltk.corpus.reader.ConllChunkCorpusReader(*args, **kwargs)\n",
    "        segmentations = dict(POS=[], chunk_type=[], sent_num=[], chunk_num=[])\n",
    "        data = []\n",
    "        for i, sent in enumerate(reader.chunked_sents()): \n",
    "            for j, chunk in enumerate(sent): \n",
    "                if type(chunk) == tuple: \n",
    "                    chunk = [chunk]\n",
    "                    chunk_type = 'punct'\n",
    "                else:\n",
    "                    chunk_type = chunk.label()\n",
    "                for word, POS in chunk: \n",
    "                    segmentations['POS'].append(POS)\n",
    "                    segmentations['chunk_type'].append(chunk_type)\n",
    "                    segmentations['sent_num'].append(i)\n",
    "                    segmentations['chunk_num'].append(j)\n",
    "                    data.append(word)\n",
    "        vcb = Vocab.build(flat_tokens=data, min_count=min_count, \n",
    "                          unk_token=unk_token)\n",
    "        data_idx = RestartableMapIterator(data, vcb.encode_token)\n",
    "        corpus = SegmentedCorpus(data=data_idx, \n",
    "                                 segmentation=segmentations, \n",
    "                                 packed=False, vocab=vcb)\n",
    "        return corpus\n",
    "    \n",
    "corpus = SegmentedCorpus.build_conll(root='../corpora/conll2000/', fileids=['test.txt'], chunk_types=None)\n",
    "corpus.list_available_segmentations()\n",
    "corpus.save('connl.pkl')\n",
    "corpus = SegmentedCorpus.load('connl.pkl')\n",
    "for segments in itertools.islice(corpus.segments_wrt(('chunk_type', 'chunk_num'), 'POS'), 3): \n",
    "    print(segments['label_coarse'])\n",
    "    for segment in segments['segments']:\n",
    "        print(\"\\t\", segment['label_fine'], corpus.vocab.decode_sent(segment['data']))\n",
    "        \n",
    "corpus = SegmentedCorpus.build_from_lines(['a b c d e', 'f g h'])\n",
    "for line in corpus.segments(None):\n",
    "    print(line)\n",
    "for line in corpus.segments('line_num'):\n",
    "    print(line)\n",
    "print()\n",
    "\n",
    "s0 = [1,1,1,1,1,1,1,1,1]\n",
    "s1 = [1,1,1,1,2,2,3,3,3]\n",
    "s2 = [1,1,2,3,4,4,4,4,5]\n",
    "seq = range(len(s1))\n",
    "vcb = Vocab.build(seq)\n",
    "sc = SegmentedCorpus(seq, [s0, s1, s2], False, vcb)\n",
    "for seg in sc.segments_wrt((0, 1), 2): \n",
    "    print(seg)\n",
    "for seg in sc.segments(0): \n",
    "    print(seg)\n",
    "\n",
    "s0 = [(1,9)]\n",
    "s1 = [(1,4), (2,2), (3,3)]\n",
    "s2 = [(1,2), (2,1), (3,1), (4,4), (5,1)]\n",
    "seq = range(9)\n",
    "vcb = Vocab.build(seq)\n",
    "sc = SegmentedCorpus(seq, [s0, s1, s2], True, vcb)\n",
    "print()\n",
    "for seg in sc.segments_wrt((0, 1), 2): \n",
    "    print(seg)\n",
    "for seg in sc.segments(0): \n",
    "    print(seg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
