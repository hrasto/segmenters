{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POS', 'chunk_type', 'sent_num', 'chunk_num']\n",
      "(0, 'NP')\n",
      "\t ('NNP', 0, 'NP') ['Rockwell', 'International', 'Corp.']\n",
      "(1, 'NP')\n",
      "\t ('POS', 1, 'NP') [\"'s\"]\n",
      "\t ('NNP', 1, 'NP') ['Tulsa']\n",
      "\t ('NN', 1, 'NP') ['unit']\n",
      "(2, 'VP')\n",
      "\t ('VBD', 2, 'VP') ['said']\n",
      "sent. (0,)\n",
      "chunk (0, 0)\n",
      "['Rockwell', 'International', 'Corp.']\n",
      "chunk (0, 1)\n",
      "[\"'s\", 'Tulsa', 'unit']\n",
      "chunk (0, 2)\n",
      "['said']\n",
      "chunk (0, 3)\n",
      "['it']\n",
      "chunk (0, 4)\n",
      "['signed']\n",
      "chunk (0, 5)\n",
      "['a', 'tentative', 'agreement']\n",
      "chunk (0, 6)\n",
      "['extending']\n",
      "chunk (0, 7)\n",
      "['its', 'contract']\n",
      "chunk (0, 8)\n",
      "['with']\n",
      "chunk (0, 9)\n",
      "['Boeing', 'Co.']\n",
      "chunk (0, 10)\n",
      "['to', 'provide']\n",
      "chunk (0, 11)\n",
      "['structural', 'parts']\n",
      "chunk (0, 12)\n",
      "['for']\n",
      "chunk (0, 13)\n",
      "['Boeing']\n",
      "chunk (0, 14)\n",
      "[\"'s\", '747', 'jetliners']\n",
      "chunk (0, 15)\n",
      "['.']\n",
      "sent. (1,)\n",
      "chunk (1, 0)\n",
      "['Rockwell']\n",
      "chunk (1, 1)\n",
      "['said']\n",
      "chunk (1, 2)\n",
      "['the', 'agreement']\n",
      "chunk (1, 3)\n",
      "['calls']\n",
      "chunk (1, 4)\n",
      "['for']\n",
      "chunk (1, 5)\n",
      "['it']\n",
      "chunk (1, 6)\n",
      "['to', 'supply']\n",
      "chunk (1, 7)\n",
      "['200', 'additional', 'so-called', 'shipsets']\n",
      "chunk (1, 8)\n",
      "['for']\n",
      "chunk (1, 9)\n",
      "['the', 'planes']\n",
      "chunk (1, 10)\n",
      "['.']\n",
      "sent. (2,)\n",
      "chunk (2, 0)\n",
      "['These']\n",
      "chunk (2, 1)\n",
      "['include']\n",
      "chunk (2, 2)\n",
      "[',']\n",
      "chunk (2, 3)\n",
      "['among']\n",
      "chunk (2, 4)\n",
      "['other', 'parts']\n",
      "chunk (2, 5)\n",
      "[',']\n",
      "chunk (2, 6)\n",
      "['each', 'jetliner']\n",
      "chunk (2, 7)\n",
      "[\"'s\", 'two', 'major', 'bulkheads']\n",
      "chunk (2, 8)\n",
      "[',']\n",
      "chunk (2, 9)\n",
      "['a', 'pressure', 'floor']\n",
      "chunk (2, 10)\n",
      "[',']\n",
      "chunk (2, 11)\n",
      "['torque', 'box']\n",
      "chunk (2, 12)\n",
      "[',']\n",
      "chunk (2, 13)\n",
      "['fixed', 'leading', 'edges']\n",
      "chunk (2, 14)\n",
      "['for']\n",
      "chunk (2, 15)\n",
      "['the', 'wings']\n",
      "chunk (2, 16)\n",
      "['and']\n",
      "chunk (2, 17)\n",
      "['an', 'aft', 'keel', 'beam']\n",
      "chunk (2, 18)\n",
      "['.']\n",
      "sent. (3,)\n",
      "chunk (3, 0)\n",
      "['Under']\n",
      "chunk (3, 1)\n",
      "['the', 'existing', 'contract']\n",
      "chunk (3, 2)\n",
      "[',']\n",
      "chunk (3, 3)\n",
      "['Rockwell']\n",
      "chunk (3, 4)\n",
      "['said']\n",
      "chunk (3, 5)\n",
      "[',']\n",
      "chunk (3, 6)\n",
      "['it']\n",
      "chunk (3, 7)\n",
      "['has', 'already', 'delivered']\n",
      "chunk (3, 8)\n",
      "['793']\n",
      "chunk (3, 9)\n",
      "['of']\n",
      "chunk (3, 10)\n",
      "['the', 'shipsets']\n",
      "chunk (3, 11)\n",
      "['to']\n",
      "chunk (3, 12)\n",
      "['Boeing']\n",
      "chunk (3, 13)\n",
      "['.']\n",
      "sent. (4,)\n",
      "chunk (4, 0)\n",
      "['Rockwell']\n",
      "chunk (4, 1)\n",
      "[',']\n",
      "chunk (4, 2)\n",
      "['based']\n",
      "chunk (4, 3)\n",
      "['in']\n",
      "chunk (4, 4)\n",
      "['El', 'Segundo']\n",
      "chunk (4, 5)\n",
      "[',']\n",
      "chunk (4, 6)\n",
      "['Calif.']\n",
      "chunk (4, 7)\n",
      "[',']\n",
      "chunk (4, 8)\n",
      "['is']\n",
      "chunk (4, 9)\n",
      "['an', 'aerospace', ',', 'electronics', ',', 'automotive', 'and', 'graphics', 'concern']\n",
      "chunk (4, 10)\n",
      "['.']\n",
      "['hello'] 0\n",
      "['there'] 1\n",
      "['how'] 2\n",
      "['are'] 3\n",
      "['you'] 4\n",
      "['?'] 5\n",
      "['hello', 'there'] (0,)\n",
      "['how', 'are', 'you', '?'] (1,)\n",
      "\n",
      "{'segments': [{'data': [0, 1], 'label_fine': (1, 1, 1)}, {'data': [2], 'label_fine': (1, 1, 2)}, {'data': [3], 'label_fine': (1, 1, 3)}], 'label_coarse': (1, 1)}\n",
      "{'segments': [{'data': [4, 5], 'label_fine': (1, 2, 4)}], 'label_coarse': (1, 2)}\n",
      "{'segments': [{'data': [6, 7], 'label_fine': (1, 3, 4)}, {'data': [8], 'label_fine': (1, 3, 5)}], 'label_coarse': (1, 3)}\n",
      "{'data': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'label': (1,)}\n",
      "\n",
      "{'segments': [{'data': [0, 1], 'label_fine': (1, 1, 1)}, {'data': [2], 'label_fine': (1, 1, 2)}, {'data': [3], 'label_fine': (1, 1, 3)}], 'label_coarse': (1, 1)}\n",
      "{'segments': [{'data': [4, 5], 'label_fine': (1, 2, 4)}], 'label_coarse': (1, 2)}\n",
      "{'segments': [{'data': [6, 7], 'label_fine': (1, 3, 4)}, {'data': [8], 'label_fine': (1, 3, 5)}], 'label_coarse': (1, 3)}\n",
      "{'data': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'label': (1,)}\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "from bidict import bidict\n",
    "import collections\n",
    "import nltk\n",
    "import pickle\n",
    "import itertools\n",
    "import nltk\n",
    "\n",
    "try: \n",
    "    from .iterator import *\n",
    "except ImportError: \n",
    "    from segtool import RestartableMapIterator, RestartableFlattenIterator, RestartableBatchIterator, RestartableCallableIterator\n",
    "\n",
    "default_unk_token = '<UNK>'\n",
    "\n",
    "class Vocab:\n",
    "    word_to_idx: bidict\n",
    "    word_to_count: dict\n",
    "    \n",
    "    def __init__(self, word_to_idx: bidict, word_to_count: dict, unk_token=default_unk_token):\n",
    "        self.word_to_count=word_to_count\n",
    "        self.word_to_idx=word_to_idx\n",
    "        self.unk_token=unk_token\n",
    "        try: \n",
    "            self.add_type(unk_token)\n",
    "        except ValueError: \n",
    "            pass\n",
    "\n",
    "    def __str__(self): \n",
    "        return str(self.word_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "    def _get_new_word_id(self): \n",
    "        return max(self.word_to_idx.values())+1\n",
    "\n",
    "    def add_type(self, token, count=0):\n",
    "        if token in self.word_to_idx: \n",
    "            raise ValueError(f\"token '{token}' is already in the vocabulary\")\n",
    "        self.word_to_idx[token] = self._get_new_word_id()\n",
    "        self.word_to_count[token] = count\n",
    "        return token\n",
    "\n",
    "    def token_count(self):\n",
    "        return int(sum(self.word_to_count.values()))\n",
    "\n",
    "    def encode_token(self, token):\n",
    "        return self.word_to_idx.get(token, self.word_to_idx[self.unk_token])\n",
    "    \n",
    "    def encode_sent(self, sent):\n",
    "        return [self.encode_token(token) for token in sent]\n",
    "\n",
    "    def encode_batch(self, sents):\n",
    "        return [self.encode_sent(sent) for sent in sents]\n",
    "\n",
    "    def decode_token(self, token_idx):\n",
    "        return self.word_to_idx.inv[token_idx]\n",
    "\n",
    "    def decode_sent(self, sent, stringify=False):\n",
    "        tokens = [self.decode_token(token) for token in sent]\n",
    "        if stringify: tokens = ' '.join(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def decode_batch(self, sents, stringify=False):\n",
    "        return [self.decode_sent(sent, stringify) for sent in sents]\n",
    "    \n",
    "    def build(flat_tokens:typing.Iterable=[], min_count=1, unk_token:str=default_unk_token):\n",
    "        \"\"\" helper method to build a vocabulary from a stream of tokens \"\"\"\n",
    "        word_to_count = collections.Counter([tok for tok in flat_tokens])\n",
    "        word_to_count = collections.Counter({w: c for w, c in word_to_count.items() if c >= min_count})\n",
    "        word_to_idx = bidict((word, i) for i, (word, count) in enumerate(word_to_count.most_common()))\n",
    "        return Vocab(word_to_idx=word_to_idx, word_to_count=word_to_count, unk_token=unk_token)\n",
    "    \n",
    "#vcb = Vocab.build('salkjdfhaszlkjdfhaskljdfhaslkjdfhajksdfhadsjklfhaslkjdfhasdkljfhasdlfj', min_count=1)\n",
    "#vcb.word_to_idx, vcb.word_to_count, vcb.encode_sent('blabla')\n",
    "    \n",
    "Key = typing.Union[str, int, typing.Set[typing.Union[str, int]]]\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, data: typing.Iterable, \n",
    "                 segmentation: typing.Union[None, typing.Iterable, typing.List[typing.Iterable], typing.Dict[str, typing.Iterable]]=None,\n",
    "                 packed=True,\n",
    "                 vocab: Vocab=None) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            vocab (Vocab): Vocabulary object (use build_vocab function to obtain it)\n",
    "            data (typing.Iterable): Any iterable object\n",
    "            segmentation (typing.Union[None, typing.Iterable, typing.List[typing.Iterable], typing.Dict[str, typing.Iterable]], optional): Segmentations. Can be a dict, list, or a single segmentation. Defaults to None.\n",
    "            packed (bool, optional): If true, indicates that in the provided segmentation format a single element is a tuple (segment_label, segment_size). Else, assumes segmentations are lists of labels where a consequtive sequence indicates a segment. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.packed = packed\n",
    "        if type(segmentation) == list: \n",
    "            self.segmentations = {i: s for i, s in enumerate(segmentation)}\n",
    "        elif type(segmentation) == dict: \n",
    "            self.segmentations = segmentation\n",
    "        else: \n",
    "            self.segmentations = {0: segmentation}\n",
    "\n",
    "    def list_available_segmentations(self):\n",
    "        return list(self.segmentations.keys())\n",
    "    \n",
    "    def _enumerate_iterables(self): \n",
    "        if type(self.data) != list:\n",
    "            self.data = list(self.data)\n",
    "        for key in self.segmentations.keys(): \n",
    "            if type(self.segmentations[key]) != list: \n",
    "                self.segmentations[key] = list(self.segmentations[key])\n",
    "    \n",
    "    def _normalize_key(self, key: Key):\n",
    "        if key is None: \n",
    "            return key\n",
    "        if type(key) == str or type(key) == int: \n",
    "            key = set([key])\n",
    "        key = set([_ for _ in key if _ is not None])\n",
    "        for single in key: \n",
    "            if single not in self.segmentations: \n",
    "                raise KeyError(f'provided key (single) not in available segmentations ({\",\".join(self.list_available_segmentations())})')\n",
    "        return key\n",
    "    \n",
    "    def _default_segmentation(self): \n",
    "        data_len = sum(1 for _ in self.data)\n",
    "        return map(lambda i: (i, 1), range(data_len))\n",
    "    \n",
    "    def _resolve_segmentation(self, *keys): \n",
    "        \"\"\" normalizes key and returns a (packed) segmentation iterator \"\"\"\n",
    "        if keys[0] is None: \n",
    "            return self._default_segmentation()\n",
    "        else: \n",
    "            keys = [self._normalize_key(k) for k in keys if k is not None]\n",
    "            key = set.union(*keys)\n",
    "            segmentations_single = [self.segmentations[k] for k in key]\n",
    "            if self.packed: \n",
    "                # for zipping, segmentations must be unpacked\n",
    "                segmentations_single = [Corpus._unpack(s) for s in segmentations_single]\n",
    "            segmentation = zip(*segmentations_single) # combine segmentations by zipping\n",
    "            segmentation = Corpus._pack(segmentation) # pack again\n",
    "            return segmentation\n",
    "    \n",
    "    def _unpack(segmentation): \n",
    "        for label, size in segmentation: \n",
    "            for i in range(size): \n",
    "                yield label\n",
    "\n",
    "    def _pack(segmentation): \n",
    "        for key, group in itertools.groupby(segmentation): \n",
    "            yield key, sum(1 for _ in group)\n",
    "\n",
    "    def segments(self, coarse: Key, fine: Key=None):\n",
    "        seg_coarse = self._resolve_segmentation(coarse)\n",
    "        iter_data = iter(self.data)\n",
    "\n",
    "        if fine is None: \n",
    "            for label, size in seg_coarse:\n",
    "                _data = [next(iter_data) for i in range(size)]\n",
    "                segment = {'data': _data, 'label': label}\n",
    "                yield segment\n",
    "        else: \n",
    "            seg_fine = self._resolve_segmentation(fine, coarse)\n",
    "            iter_seg_fine = iter(seg_fine)\n",
    "            iter_seg_coarse = iter(seg_coarse)\n",
    "            while True: \n",
    "                try: key_coarse, size_coarse = next(iter_seg_coarse)\n",
    "                except StopIteration: break\n",
    "                segment = []\n",
    "                while size_coarse > 0: \n",
    "                    key_fine, size_fine = next(iter_seg_fine)\n",
    "                    segment.append({\n",
    "                        'data': [next(iter_data) for i in range(size_fine)], \n",
    "                        'label_fine': key_fine})\n",
    "                    size_coarse -= size_fine\n",
    "                yield {'segments': segment, 'label_coarse': key_coarse}        \n",
    "\n",
    "    def save(self, path, enumerate_iterables=True): \n",
    "        if enumerate_iterables: self._enumerate_iterables()\n",
    "        with open(path, 'wb') as f: \n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def load(path):\n",
    "        with open(path, 'rb') as f: \n",
    "            return pickle.load(f)\n",
    "\n",
    "    def build_from_lines(lines: typing.Iterable, split_line=str.split, line_index=True, min_count=1, unk_token=default_unk_token): \n",
    "        \"\"\"Build corpus from lines.\n",
    "\n",
    "        Args:\n",
    "            lines (typing.Iterable): Iterable over strings that can be split by split_line.\n",
    "            split_line (_type_, optional): Function that splits lines. Defaults to str.spl\n",
    "            line_index (bool, optional): Whether to include a line index as a segmentation. Defaults to True.\n",
    "            min_count (int, optional): Minimum word count for vocabulary building. Defaults to 1.\n",
    "            unk_token (_type_, optional): Unknown token. Defaults to '<UNK>'.\n",
    "\n",
    "        Returns:\n",
    "            Corpus: Built corpus.\n",
    "        \"\"\"\n",
    "        lines_split = RestartableMapIterator(lines, split_line)\n",
    "        lines_split_flat = RestartableFlattenIterator(lines_split)\n",
    "        vcb = Vocab.build(flat_tokens=lines_split_flat, \n",
    "                          min_count=min_count, unk_token=unk_token)\n",
    "        lines_split_flat_idx = RestartableMapIterator(\n",
    "            lines_split_flat, vcb.encode_token)\n",
    "        segmentations={}\n",
    "        if line_index: \n",
    "            segmentations['line_num'] = []\n",
    "            for i, line in enumerate(lines_split):\n",
    "                segmentations['line_num'].append((i, len(line)))\n",
    "        corpus = Corpus(data=lines_split_flat_idx, \n",
    "                                 segmentation=segmentations, \n",
    "                                 packed=True, vocab=vcb)\n",
    "        return corpus\n",
    "\n",
    "    def build_conll(min_count=1, unk_token=default_unk_token, *args, **kwargs): \n",
    "        \"\"\"Builds corpus from CoNLL formatted file(s). \n",
    "\n",
    "        Args:\n",
    "            min_count (int, optional): Minimum word count to consider when building vocabulary. Defaults to 1.\n",
    "            unk_token (_type_, optional): Unknown token. Defaults to '<UNK>'.\n",
    "\n",
    "        Returns:\n",
    "            Corpus: Built corpus.\n",
    "        \"\"\"\n",
    "        reader = nltk.corpus.reader.ConllChunkCorpusReader(*args, **kwargs)\n",
    "        segmentations = dict(POS=[], chunk_type=[], sent_num=[], chunk_num=[])\n",
    "        data = []\n",
    "        for i, sent in enumerate(reader.chunked_sents()): \n",
    "            for j, chunk in enumerate(sent): \n",
    "                if type(chunk) == tuple: \n",
    "                    chunk = [chunk]\n",
    "                    chunk_type = 'punct'\n",
    "                else:\n",
    "                    chunk_type = chunk.label()\n",
    "                for word, POS in chunk: \n",
    "                    segmentations['POS'].append(POS)\n",
    "                    segmentations['chunk_type'].append(chunk_type)\n",
    "                    segmentations['sent_num'].append(i)\n",
    "                    segmentations['chunk_num'].append(j)\n",
    "                    data.append(word)\n",
    "        vcb = Vocab.build(flat_tokens=data, min_count=min_count, \n",
    "                          unk_token=unk_token)\n",
    "        data_idx = RestartableMapIterator(data, vcb.encode_token)\n",
    "        corpus = Corpus(data=data_idx, \n",
    "                                 segmentation=segmentations, \n",
    "                                 packed=False, vocab=vcb)\n",
    "        return corpus\n",
    "    \n",
    "corpus = Corpus.build_conll(root='../corpora/conll2000/', fileids=['test.txt'], chunk_types=None)\n",
    "print(corpus.list_available_segmentations())\n",
    "corpus.save('connl.pkl')\n",
    "corpus = Corpus.load('connl.pkl')\n",
    "for segments in itertools.islice(corpus.segments(('chunk_type', 'chunk_num'), 'POS'), 3): \n",
    "    print(segments['label_coarse'])\n",
    "    for segment in segments['segments']:\n",
    "        print(\"\\t\", segment['label_fine'], corpus.vocab.decode_sent(segment['data']))\n",
    "for Seg in itertools.islice(corpus.segments(coarse='sent_num', fine='chunk_num'), 5): \n",
    "    print(f'sent. {Seg[\"label_coarse\"]}')\n",
    "    for seg in Seg['segments']: \n",
    "        print(f'chunk {seg[\"label_fine\"]}')\n",
    "        print(corpus.vocab.decode_sent(seg['data']))\n",
    "        \n",
    "corpus = Corpus.build_from_lines([\n",
    "    'hello there', \n",
    "    'how are you ?',\n",
    "], split_line=str.split, min_count=1, unk_token='<UNK>')\n",
    "\n",
    "for line in corpus.segments(None):\n",
    "    print(corpus.vocab.decode_sent(line['data']), line['label'])\n",
    "for line in corpus.segments('line_num'):\n",
    "    print(corpus.vocab.decode_sent(line['data']), line['label'])\n",
    "print()\n",
    "\n",
    "s0 = [1,1,1,1,1,1,1,1,1]\n",
    "s1 = [1,1,1,1,2,2,3,3,3]\n",
    "s2 = [1,1,2,3,4,4,4,4,5]\n",
    "seq = range(len(s1))\n",
    "vcb = Vocab.build(seq)\n",
    "sc = Corpus(seq, [s0, s1, s2], False, vcb)\n",
    "for seg in sc.segments((0, 1), 2): \n",
    "    print(seg)\n",
    "for seg in sc.segments(0): \n",
    "    print(seg)\n",
    "\n",
    "s0 = [(1,9)]\n",
    "s1 = [(1,4), (2,2), (3,3)]\n",
    "s2 = [(1,2), (2,1), (3,1), (4,4), (5,1)]\n",
    "seq = range(9)\n",
    "vcb = Vocab.build(seq)\n",
    "sc = Corpus(seq, [s0, s1, s2], True, vcb)\n",
    "print()\n",
    "for seg in sc.segments((0, 1), 2): \n",
    "    print(seg)\n",
    "for seg in sc.segments(0): \n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml_env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import segtool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = segtool.Corpus.build_from_lines([\n",
    "    'hello there',\n",
    "    'how are you ?'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['line_num']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.list_available_segmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [0, 1], 'label': (0,)}\n",
      "{'data': [2, 3, 4, 5], 'label': (1,)}\n"
     ]
    }
   ],
   "source": [
    "for seg in corpus.segments('line_num'): \n",
    "    print(seg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
