{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from bidict import bidict\n",
    "import collections\n",
    "import nltk\n",
    "import pickle\n",
    "import itertools\n",
    "import nltk\n",
    "\n",
    "try: \n",
    "    from .iterator import *\n",
    "except ImportError: \n",
    "    from segtool import RestartableMapIterator, RestartableFlattenIterator, RestartableBatchIterator, RestartableCallableIterator\n",
    "\n",
    "default_unk_token = '<UNK>'\n",
    "\n",
    "class Vocab:\n",
    "    word_to_idx: bidict\n",
    "    word_to_count: dict\n",
    "    \n",
    "    def __init__(self, word_to_idx: bidict, word_to_count: dict, unk_token=default_unk_token):\n",
    "        self.word_to_count=word_to_count\n",
    "        self.word_to_idx=word_to_idx\n",
    "        self.unk_token=unk_token\n",
    "        try: \n",
    "            self.add_type(unk_token)\n",
    "        except ValueError: \n",
    "            pass\n",
    "\n",
    "    def __str__(self): \n",
    "        return str(self.word_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "    def _get_new_word_id(self): \n",
    "        return max(self.word_to_idx.values())+1\n",
    "\n",
    "    def add_type(self, token, count=0):\n",
    "        if token in self.word_to_idx: \n",
    "            raise ValueError(f\"token '{token}' is already in the vocabulary\")\n",
    "        self.word_to_idx[token] = self._get_new_word_id()\n",
    "        self.word_to_count[token] = count\n",
    "        return token\n",
    "\n",
    "    def token_count(self):\n",
    "        return int(sum(self.word_to_count.values()))\n",
    "\n",
    "    def encode_token(self, token):\n",
    "        return self.word_to_idx.get(token, self.word_to_idx[self.unk_token])\n",
    "    \n",
    "    def encode_sent(self, sent):\n",
    "        return [self.encode_token(token) for token in sent]\n",
    "\n",
    "    def encode_batch(self, sents):\n",
    "        return [self.encode_sent(sent) for sent in sents]\n",
    "\n",
    "    def decode_token(self, token_idx):\n",
    "        return self.word_to_idx.inv[token_idx]\n",
    "\n",
    "    def decode_sent(self, sent, stringify=False):\n",
    "        tokens = [self.decode_token(token) for token in sent]\n",
    "        if stringify: tokens = ' '.join(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def decode_batch(self, sents, stringify=False):\n",
    "        return [self.decode_sent(sent, stringify) for sent in sents]\n",
    "    \n",
    "    def build(flat_tokens:typing.Iterable=[], min_count=1, unk_token:str=default_unk_token):\n",
    "        \"\"\" helper method to build a vocabulary from a stream of tokens \"\"\"\n",
    "        word_to_count = collections.Counter([tok for tok in flat_tokens])\n",
    "        word_to_count = collections.Counter({w: c for w, c in word_to_count.items() if c >= min_count})\n",
    "        word_to_idx = bidict((word, i) for i, (word, count) in enumerate(word_to_count.most_common()))\n",
    "        return Vocab(word_to_idx=word_to_idx, word_to_count=word_to_count, unk_token=unk_token)\n",
    "    \n",
    "#vcb = Vocab.build('salkjdfhaszlkjdfhaskljdfhaslkjdfhajksdfhadsjklfhaslkjdfhasdkljfhasdlfj', min_count=1)\n",
    "#vcb.word_to_idx, vcb.word_to_count, vcb.encode_sent('blabla')\n",
    "    \n",
    "Key = typing.Union[str, int, typing.Set[typing.Union[str, int]]]\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, data: typing.Iterable, \n",
    "                 segmentation: typing.Union[None, typing.Iterable, typing.List[typing.Iterable], typing.Dict[str, typing.Iterable]]=None,\n",
    "                 packed=True,\n",
    "                 vocab: Vocab=None) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            vocab (Vocab): Vocabulary object (use build_vocab function to obtain it)\n",
    "            data (typing.Iterable): Any iterable object\n",
    "            segmentation (typing.Union[None, typing.Iterable, typing.List[typing.Iterable], typing.Dict[str, typing.Iterable]], optional): Segmentations. Can be a dict, list, or a single segmentation. Defaults to None.\n",
    "            packed (bool, optional): If true, indicates that in the provided segmentation format a single element is a tuple (segment_label, segment_size). Else, assumes segmentations are lists of labels where a consequtive sequence indicates a segment. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.packed = packed\n",
    "        if type(segmentation) == list: \n",
    "            self.segmentations = {i: s for i, s in enumerate(segmentation)}\n",
    "        elif type(segmentation) == dict: \n",
    "            self.segmentations = segmentation\n",
    "        else: \n",
    "            self.segmentations = {0: segmentation}\n",
    "\n",
    "    def list_available_segmentations(self):\n",
    "        return list(self.segmentations.keys())\n",
    "    \n",
    "    def _enumerate_iterables(self): \n",
    "        if type(self.data) != list:\n",
    "            self.data = list(self.data)\n",
    "        for key in self.segmentations.keys(): \n",
    "            if type(self.segmentations[key]) != list: \n",
    "                self.segmentations[key] = list(self.segmentations[key])\n",
    "    \n",
    "    def _normalize_key(self, key: Key):\n",
    "        if key is None: \n",
    "            return key\n",
    "        if type(key) == str or type(key) == int: \n",
    "            key = set([key])\n",
    "        key = set([_ for _ in key if _ is not None])\n",
    "        for single in key: \n",
    "            if single not in self.segmentations: \n",
    "                raise KeyError(f'provided key (single) not in available segmentations ({\",\".join(self.list_available_segmentations())})')\n",
    "        return key\n",
    "    \n",
    "    def _normalize_keys(self, *keys: typing.Tuple[Key]) -> Key:\n",
    "        keys = [self._normalize_key(k) for k in keys if k is not None]\n",
    "        key = set.union(*keys)\n",
    "        return key\n",
    "\n",
    "    def _unpack(segmentation): \n",
    "        for label, size in segmentation: \n",
    "            for i in range(size): \n",
    "                yield label\n",
    "\n",
    "    def _pack(segmentation): \n",
    "        for key, group in itertools.groupby(segmentation): \n",
    "            yield key, sum(1 for _ in group)\n",
    "\n",
    "    def _default_segmentation(self): \n",
    "        data_len = sum(1 for _ in self.data)\n",
    "        return map(lambda i: (i, 1), range(data_len))\n",
    "    \n",
    "    def _resolve_segmentation(self, *keys): \n",
    "        \"\"\" normalizes key and returns a (packed) segmentation iterator \"\"\"\n",
    "        if None in keys: \n",
    "            return self._default_segmentation()\n",
    "        else: \n",
    "            key = self._normalize_keys(*keys)\n",
    "            segmentations_single = [self.segmentations[k] for k in key]\n",
    "            if self.packed: \n",
    "                # for zipping, segmentations must be unpacked\n",
    "                segmentations_single = [Corpus._unpack(s) for s in segmentations_single]\n",
    "            segmentation = zip(*segmentations_single) # combine segmentations by zipping\n",
    "            segmentation = Corpus._pack(segmentation) # pack again\n",
    "            return segmentation    \n",
    "\n",
    "    def _resolve_segmentation_adjusted(self, coarse: Key, fine: Key): \n",
    "        \"\"\" iterates over coarse, but sizes are adjusted relative to the number of corresponding subsegments in fine (rather than elements in data) \"\"\"\n",
    "        seg_fine = self._resolve_segmentation(coarse, fine)\n",
    "        iter_seg_fine = iter(seg_fine)\n",
    "        seg_coarse = self._resolve_segmentation(coarse)\n",
    "        for label_c, size_c in seg_coarse: \n",
    "            size_c_accum = 0\n",
    "            num_fine_segments = 0\n",
    "            while size_c_accum < size_c: \n",
    "                _, size_f = next(iter_seg_fine)\n",
    "                size_c_accum += size_f\n",
    "                num_fine_segments += 1\n",
    "            yield label_c, num_fine_segments\n",
    "\n",
    "    def segments(self, *segmentations: typing.Tuple[Key]):\n",
    "        \"\"\" segmentations should be in the order of coarse -> fine \"\"\"\n",
    "        # normalize segmentations (each entry will be a set)\n",
    "        segmentation_keys = [self._normalize_keys(key) for key in segmentations]\n",
    "        # adjust the keys such that finer segmentation always contain the (boundaries of) coarser segmentations\n",
    "        segmentation_keys_cumul = [set.union(*segmentation_keys[:i]) for i in range(1, len(segmentation_keys)+1)]\n",
    "        segmentation_keys_cumul.append(None)\n",
    "        coarses = segmentation_keys_cumul[:-1]\n",
    "        fines = segmentation_keys_cumul[1:]\n",
    "        # get the (relative) segmentation iterables \n",
    "        segmentation_iterables_adj = [self._resolve_segmentation_adjusted(c, f) for c, f in zip(coarses, fines)]\n",
    "        coarsest = segmentation_iterables_adj[0]\n",
    "        fines_adj = segmentation_iterables_adj[1:]\n",
    "        iter_fines = [iter(f) for f in fines_adj]\n",
    "        data_iter = iter(self.data)\n",
    "\n",
    "        def consume_iters(data, label, num, *iters): \n",
    "            if len(iters) == 0: \n",
    "                _data = list(itertools.islice(data, num))\n",
    "                return dict(data=_data, label=label)\n",
    "            else: \n",
    "                _data = [consume_iters(data, *el, *iters[1:]) \n",
    "                        for el in itertools.islice(iters[0], num)]\n",
    "                return dict(data=_data, label=label)\n",
    "\n",
    "        for label, size in coarsest:\n",
    "            # call helper that recursively builds a dictionary containing nested segmentation\n",
    "            yield consume_iters(data_iter, label, size, *iter_fines)\n",
    "\n",
    "    def save(self, path, enumerate_iterables=True): \n",
    "        if enumerate_iterables: self._enumerate_iterables()\n",
    "        with open(path, 'wb') as f: \n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def load(path):\n",
    "        with open(path, 'rb') as f: \n",
    "            return pickle.load(f)\n",
    "\n",
    "    def build_from_lines(lines: typing.Iterable, split_line=str.split, line_index=True, min_count=1, unk_token=default_unk_token): \n",
    "        \"\"\"Build corpus from lines.\n",
    "\n",
    "        Args:\n",
    "            lines (typing.Iterable): Iterable over strings that can be split by split_line.\n",
    "            split_line (_type_, optional): Function that splits lines. Defaults to str.spl\n",
    "            line_index (bool, optional): Whether to include a line index as a segmentation. Defaults to True.\n",
    "            min_count (int, optional): Minimum word count for vocabulary building. Defaults to 1.\n",
    "            unk_token (_type_, optional): Unknown token. Defaults to '<UNK>'.\n",
    "\n",
    "        Returns:\n",
    "            Corpus: Built corpus.\n",
    "        \"\"\"\n",
    "        lines_split = RestartableMapIterator(lines, split_line)\n",
    "        lines_split_flat = RestartableFlattenIterator(lines_split)\n",
    "        vcb = Vocab.build(flat_tokens=lines_split_flat, \n",
    "                          min_count=min_count, unk_token=unk_token)\n",
    "        lines_split_flat_idx = RestartableMapIterator(\n",
    "            lines_split_flat, vcb.encode_token)\n",
    "        segmentations={}\n",
    "        if line_index: \n",
    "            segmentations['line_num'] = []\n",
    "            for i, line in enumerate(lines_split):\n",
    "                segmentations['line_num'].append((i, len(line)))\n",
    "        corpus = Corpus(data=lines_split_flat_idx, \n",
    "                                 segmentation=segmentations, \n",
    "                                 packed=True, vocab=vcb)\n",
    "        return corpus\n",
    "\n",
    "    def build_conll_chunk(min_count=1, unk_token=default_unk_token, *args, **kwargs): \n",
    "        \"\"\"Builds corpus from CoNLL formatted chunking file(s). \n",
    "\n",
    "        Args:\n",
    "            min_count (int, optional): Minimum word count to consider when building vocabulary. Defaults to 1.\n",
    "            unk_token (_type_, optional): Unknown token. Defaults to '<UNK>'.\n",
    "\n",
    "        Returns:\n",
    "            Corpus: Built corpus.\n",
    "        \"\"\"\n",
    "        reader = nltk.corpus.reader.ConllChunkCorpusReader(*args, **kwargs)\n",
    "        segmentations = dict(POS=[], chunk_type=[], sent_num=[], chunk_num=[])\n",
    "        data = []\n",
    "        for i, sent in enumerate(reader.chunked_sents()): \n",
    "            for j, chunk in enumerate(sent): \n",
    "                if type(chunk) == tuple: \n",
    "                    chunk = [chunk]\n",
    "                    chunk_type = 'punct'\n",
    "                else:\n",
    "                    chunk_type = chunk.label()\n",
    "                for word, POS in chunk: \n",
    "                    segmentations['POS'].append(POS)\n",
    "                    segmentations['chunk_type'].append(chunk_type)\n",
    "                    segmentations['sent_num'].append(i)\n",
    "                    segmentations['chunk_num'].append(j)\n",
    "                    data.append(word)\n",
    "        vcb = Vocab.build(flat_tokens=data, min_count=min_count, \n",
    "                          unk_token=unk_token)\n",
    "        data_idx = RestartableMapIterator(data, vcb.encode_token)\n",
    "        corpus = Corpus(data=data_idx, \n",
    "                                 segmentation=segmentations, \n",
    "                                 packed=False, vocab=vcb)\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'data': [{'data': ['t', 'h', 'e'], 'label': ('a', 'i', 'u')},\n",
      "                    {'data': ['b', 'o', 'y'], 'label': ('a', 'i', 'v')}],\n",
      "           'label': ('a', 'i')},\n",
      "          {'data': [{'data': ['s', 'a', 'i', 'd'], 'label': ('a', 'j', 'w')}],\n",
      "           'label': ('a', 'j')}],\n",
      " 'label': ('a',)}\n",
      "{'data': [{'data': [{'data': ['h', 'i'], 'label': ('b', 'k', 'x')}],\n",
      "           'label': ('b', 'k')},\n",
      "          {'data': [{'data': ['t', 'h', 'e', 'r', 'e'],\n",
      "                     'label': ('b', 'l', 'y')}],\n",
      "           'label': ('b', 'l')}],\n",
      " 'label': ('b',)}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "seq = 'theboysaidhithere'\n",
    "segmentations = {\n",
    "    0: 'aaaaaaaaaabbbbbbb',\n",
    "    1: 'iiiiiijjjjkklllll', \n",
    "    2: 'uuuvvvwwwwxxyyyyy',\n",
    "}\n",
    "sc = Corpus(seq, segmentations, packed=False)\n",
    "\n",
    "for seg in sc.segments(0,1,2): \n",
    "    pprint(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'data': ['t', 'h', 'e'], 'label': ('a', 'u')},\n",
      "          {'data': ['b', 'o', 'y'], 'label': ('a', 'v')},\n",
      "          {'data': ['s', 'a', 'i', 'd'], 'label': ('a', 'w')}],\n",
      " 'label': ('a',)}\n",
      "{'data': [{'data': ['h', 'i'], 'label': ('b', 'x')},\n",
      "          {'data': ['t', 'h', 'e', 'r', 'e'], 'label': ('b', 'y')}],\n",
      " 'label': ('b',)}\n"
     ]
    }
   ],
   "source": [
    "for seg in sc.segments(0,2): \n",
    "    pprint(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': ['t', 'h', 'e', 'b', 'o', 'y'], 'label': ('i',)}\n",
      "{'data': ['s', 'a', 'i', 'd'], 'label': ('j',)}\n",
      "{'data': ['h', 'i'], 'label': ('k',)}\n",
      "{'data': ['t', 'h', 'e', 'r', 'e'], 'label': ('l',)}\n"
     ]
    }
   ],
   "source": [
    "for seg in sc.segments(1): \n",
    "    pprint(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POS', 'chunk_type', 'sent_num', 'chunk_num']\n",
      "('NP', 0)\n",
      "\t ('NNP', 'NP', 0) ['Rockwell', 'International', 'Corp.']\n",
      "('NP', 1)\n",
      "\t ('POS', 'NP', 1) [\"'s\"]\n",
      "\t ('NNP', 'NP', 1) ['Tulsa']\n",
      "\t ('NN', 'NP', 1) ['unit']\n",
      "('VP', 2)\n",
      "\t ('VBD', 'VP', 2) ['said']\n",
      "sent. (0,)\n",
      "chunk (0, 0)\n",
      "['Rockwell', 'International', 'Corp.']\n",
      "chunk (0, 1)\n",
      "[\"'s\", 'Tulsa', 'unit']\n",
      "chunk (0, 2)\n",
      "['said']\n",
      "chunk (0, 3)\n",
      "['it']\n",
      "chunk (0, 4)\n",
      "['signed']\n",
      "chunk (0, 5)\n",
      "['a', 'tentative', 'agreement']\n",
      "chunk (0, 6)\n",
      "['extending']\n",
      "chunk (0, 7)\n",
      "['its', 'contract']\n",
      "chunk (0, 8)\n",
      "['with']\n",
      "chunk (0, 9)\n",
      "['Boeing', 'Co.']\n",
      "chunk (0, 10)\n",
      "['to', 'provide']\n",
      "chunk (0, 11)\n",
      "['structural', 'parts']\n",
      "chunk (0, 12)\n",
      "['for']\n",
      "chunk (0, 13)\n",
      "['Boeing']\n",
      "chunk (0, 14)\n",
      "[\"'s\", '747', 'jetliners']\n",
      "chunk (0, 15)\n",
      "['.']\n",
      "sent. (1,)\n",
      "chunk (1, 0)\n",
      "['Rockwell']\n",
      "chunk (1, 1)\n",
      "['said']\n",
      "chunk (1, 2)\n",
      "['the', 'agreement']\n",
      "chunk (1, 3)\n",
      "['calls']\n",
      "chunk (1, 4)\n",
      "['for']\n",
      "chunk (1, 5)\n",
      "['it']\n",
      "chunk (1, 6)\n",
      "['to', 'supply']\n",
      "chunk (1, 7)\n",
      "['200', 'additional', 'so-called', 'shipsets']\n",
      "chunk (1, 8)\n",
      "['for']\n",
      "chunk (1, 9)\n",
      "['the', 'planes']\n",
      "chunk (1, 10)\n",
      "['.']\n",
      "sent. (2,)\n",
      "chunk (2, 0)\n",
      "['These']\n",
      "chunk (2, 1)\n",
      "['include']\n",
      "chunk (2, 2)\n",
      "[',']\n",
      "chunk (2, 3)\n",
      "['among']\n",
      "chunk (2, 4)\n",
      "['other', 'parts']\n",
      "chunk (2, 5)\n",
      "[',']\n",
      "chunk (2, 6)\n",
      "['each', 'jetliner']\n",
      "chunk (2, 7)\n",
      "[\"'s\", 'two', 'major', 'bulkheads']\n",
      "chunk (2, 8)\n",
      "[',']\n",
      "chunk (2, 9)\n",
      "['a', 'pressure', 'floor']\n",
      "chunk (2, 10)\n",
      "[',']\n",
      "chunk (2, 11)\n",
      "['torque', 'box']\n",
      "chunk (2, 12)\n",
      "[',']\n",
      "chunk (2, 13)\n",
      "['fixed', 'leading', 'edges']\n",
      "chunk (2, 14)\n",
      "['for']\n",
      "chunk (2, 15)\n",
      "['the', 'wings']\n",
      "chunk (2, 16)\n",
      "['and']\n",
      "chunk (2, 17)\n",
      "['an', 'aft', 'keel', 'beam']\n",
      "chunk (2, 18)\n",
      "['.']\n",
      "sent. (3,)\n",
      "chunk (3, 0)\n",
      "['Under']\n",
      "chunk (3, 1)\n",
      "['the', 'existing', 'contract']\n",
      "chunk (3, 2)\n",
      "[',']\n",
      "chunk (3, 3)\n",
      "['Rockwell']\n",
      "chunk (3, 4)\n",
      "['said']\n",
      "chunk (3, 5)\n",
      "[',']\n",
      "chunk (3, 6)\n",
      "['it']\n",
      "chunk (3, 7)\n",
      "['has', 'already', 'delivered']\n",
      "chunk (3, 8)\n",
      "['793']\n",
      "chunk (3, 9)\n",
      "['of']\n",
      "chunk (3, 10)\n",
      "['the', 'shipsets']\n",
      "chunk (3, 11)\n",
      "['to']\n",
      "chunk (3, 12)\n",
      "['Boeing']\n",
      "chunk (3, 13)\n",
      "['.']\n",
      "sent. (4,)\n",
      "chunk (4, 0)\n",
      "['Rockwell']\n",
      "chunk (4, 1)\n",
      "[',']\n",
      "chunk (4, 2)\n",
      "['based']\n",
      "chunk (4, 3)\n",
      "['in']\n",
      "chunk (4, 4)\n",
      "['El', 'Segundo']\n",
      "chunk (4, 5)\n",
      "[',']\n",
      "chunk (4, 6)\n",
      "['Calif.']\n",
      "chunk (4, 7)\n",
      "[',']\n",
      "chunk (4, 8)\n",
      "['is']\n",
      "chunk (4, 9)\n",
      "['an', 'aerospace', ',', 'electronics', ',', 'automotive', 'and', 'graphics', 'concern']\n",
      "chunk (4, 10)\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus.build_conll_chunk(root='../corpora/conll2000/', fileids=['test.txt'], chunk_types=None)\n",
    "print(corpus.list_available_segmentations())\n",
    "corpus.save('connl.pkl')\n",
    "corpus = Corpus.load('connl.pkl')\n",
    "for segments in itertools.islice(corpus.segments(('chunk_type', 'chunk_num'), 'POS'), 3): \n",
    "    print(segments['label'])\n",
    "    for segment in segments['data']:\n",
    "        print(\"\\t\", segment['label'], corpus.vocab.decode_sent(segment['data']))\n",
    "\n",
    "for seg in itertools.islice(corpus.segments('sent_num', 'chunk_num'), 5): \n",
    "    print(f'sent. {seg[\"label\"]}')\n",
    "    for subseg in seg['data']: \n",
    "        print(f'chunk {subseg[\"label\"]}')\n",
    "        print(corpus.vocab.decode_sent(subseg['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus.build_from_lines([\n",
    "    'hello there', \n",
    "    'how are you ?',\n",
    "], split_line=str.split, min_count=1, unk_token='<UNK>')\n",
    "\n",
    "for line in corpus.segments(None):\n",
    "    print(corpus.vocab.decode_sent(line['data']), line['label'])\n",
    "for line in corpus.segments('line_num'):\n",
    "    print(corpus.vocab.decode_sent(line['data']), line['label'])\n",
    "print()\n",
    "\n",
    "s0 = [1,1,1,1,1,1,1,1,1]\n",
    "s1 = [1,1,1,1,2,2,3,3,3]\n",
    "s2 = [1,1,2,3,4,4,4,4,5]\n",
    "seq = range(len(s1))\n",
    "vcb = Vocab.build(seq)\n",
    "sc = Corpus(seq, [s0, s1, s2], False, vcb)\n",
    "for seg in sc.segments((0, 1), 2): \n",
    "    print(seg)\n",
    "for seg in sc.segments(0): \n",
    "    print(seg)\n",
    "\n",
    "s0 = [(1,9)]\n",
    "s1 = [(1,4), (2,2), (3,3)]\n",
    "s2 = [(1,2), (2,1), (3,1), (4,4), (5,1)]\n",
    "seq = range(9)\n",
    "vcb = Vocab.build(seq)\n",
    "sc = Corpus(seq, [s0, s1, s2], True, vcb)\n",
    "print()\n",
    "for seg in sc.segments((0, 1), 2): \n",
    "    print(seg)\n",
    "for seg in sc.segments(0): \n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.union(set([1,2,3]), set([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segtool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = segtool.Corpus.build_from_lines([\n",
    "    'hello there',\n",
    "    'how are you ?'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.list_available_segmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg in corpus.segments('line_num'): \n",
    "    print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
